{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.sql.SparkSession.Builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levanto los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from IPython.display import display, HTML\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils.emissions_normalizer import EmissionsNormalizer\n",
    "from utils.neural_network_predictor import NeuralNetworkPredictor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p1, p2):\n",
    "    return math.sqrt((p1[0]-p2[0])**2+(p1[1]-p2[1])**2)\n",
    "\n",
    "def get_regressor_mae(predictions, real):\n",
    "    mae = 0\n",
    "    for i in range(len(predictions)):\n",
    "        mae += distance(predictions[i], real[i])\n",
    "    mae = mae/len(predictions)\n",
    "    return mae\n",
    "\n",
    "\n",
    "global dict_coordenadas\n",
    "dict_coordenadas = points_recep.map(lambda x: (x['Punto'],(x['x'], x['y']))).collectAsMap()\n",
    "\n",
    "def get_classifier_mae(predictions, real):\n",
    "    sum_error = 0\n",
    "    count = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] not in dict_coordenadas:\n",
    "            print('predicted point dont exist {}'.format(predictions[i]))\n",
    "            continue\n",
    "        pred_position = dict_coordenadas[predictions[i]]\n",
    "        real_position = dict_coordenadas[real[i]]\n",
    "        sum_error += distance(pred_position, real_position)\n",
    "        count += 1\n",
    "    return sum_error/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_recep = sc.read.json('datos/train-test-by-emission.jsonlines/').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = EmissionsNormalizer()\n",
    "data = normalizer.normalize(points_recep)\n",
    "regre_data, regre_target = normalizer.get_regression_dataframes(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales para regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'datos/tmp/nn-model-2h'\n",
    "def k_cross_validation(predictor, data, target, k=5, init_from_file = False, save_in_file = False):   \n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    mae_list = []\n",
    "    mae_list_train = []\n",
    "    k_index = 0\n",
    "        \n",
    "    for train_index, test_index in kf.split(data):\n",
    "        predictor.clear()\n",
    "        if init_from_file:\n",
    "            predictor.open_model(MODEL_PATH)\n",
    "\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "        \n",
    "        predictor.fit(X_train, X_test)\n",
    "\n",
    "        \n",
    "        predicted = predictor.predict(X_test)\n",
    "        mae = get_regressor_mae(predictions=predicted, real=y_test)\n",
    "        print(mae)\n",
    "             \n",
    "        predicted_train = predictor.predict(X_train)\n",
    "        mae_train = get_regressor_mae(predictions=predicted_train, real=y_train)\n",
    "        print(mae_train)\n",
    "        \n",
    "        mae_list.append(mae)\n",
    "        mae_list_train.append(mae_train)\n",
    "        if save_in_file: predictor.save_model(MODEL_PATH)\n",
    "\n",
    "    return mae_list, mae_list_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_cross_validation(NeuralNetworkPredictor(), regre_data, regre_target, k=5, init_from_file=False, save_in_file=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales para clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_target = pd.DataFrame(all_emissions.map(lambda x: x['point']).collect())\n",
    "classi_data = pd.DataFrame(all_emissions.map(lambda x: x['data']).collect())\n",
    "\n",
    "\n",
    "classi_data_np = np.array(classi_data.astype(float))\n",
    "classi_target_np = np.array(classi_target.astype(int))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(classi_data_np, classi_target_np, test_size=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "output_dim = 543\n",
    "\n",
    "model = LinearClassificationModel(input_dim,output_dim)\n",
    "\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "l_rate = 0.01\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr = l_rate) #Stochastic Gradient Descent\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr=l_rate,weight_decay=1e-4)\n",
    "\n",
    "\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch +=1\n",
    "    #increase the number of epochs by 1 every time\n",
    "    \n",
    "    inputs = Variable(torch.Tensor(X_train), requires_grad=False)\n",
    "    labels = Variable(torch.Tensor(y_train).long(), requires_grad=False)\n",
    "    \n",
    "\n",
    "    #clear grads as discussed in prev post\n",
    "    optimiser.zero_grad()\n",
    "    #forward to get predicted values\n",
    "    outputs = model.forward(inputs)\n",
    "    loss = criterion(outputs, labels.view(-1))\n",
    "    loss.backward()# back props\n",
    "    optimiser.step()# update the parameters\n",
    "    print('epoch {}, loss {}'.format(epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Variable(torch.Tensor(X_test), requires_grad=False)\n",
    "predicted_proba = model.forward(test)\n",
    "predicted_proba = predicted_proba.exp().detach().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for probas in predicted_proba:\n",
    "    point = np.argmax(probas)\n",
    "    predicted.append(point)\n",
    "\n",
    "get_classifier_mae(predictions=predicted, real=y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classi_target_np[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.unique(classi_target_np, axis=0) == classi_target_np[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_cross_validation_classi(model_builder, data, target, r_target_np, k=5):   \n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    mae_list = []\n",
    "    mae_list_train = []\n",
    "    k_index = 0\n",
    "    \n",
    "    unique_target = np.unique(target, axis=0)\n",
    "    groups = []\n",
    "    for h in range(len(target)):\n",
    "        i,j = np.where(unique_target == target[h])\n",
    "        groups.append(i[0])\n",
    "    \n",
    "    for train_index, test_index in kf.split(data, groups=groups):\n",
    "        regressor = KNeighborsRegressor()\n",
    "        params = model_builder()\n",
    "        model = params['model']\n",
    "        l_rate = params['l_rate']\n",
    "        optimiser = params['optimiser']\n",
    "        criterion = params['criterion']\n",
    "        epochs = params['epochs']\n",
    "\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            epoch +=1\n",
    "            #increase the number of epochs by 1 every time\n",
    "\n",
    "            inputs = Variable(torch.Tensor(X_train), requires_grad=False)\n",
    "            labels = Variable(torch.Tensor(y_train).long(), requires_grad=False)\n",
    "\n",
    "\n",
    "            #clear grads as discussed in prev post\n",
    "            optimiser.zero_grad()\n",
    "            #forward to get predicted values\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels.view(-1))\n",
    "            loss.backward()# back props\n",
    "            optimiser.step()# update the parameters\n",
    "            if k_index == 0 and epoch < 1000: print('epoch {}, loss {}'.format(epoch,loss.item()))\n",
    "            if k_index == 0 and epoch > 1000 and random.randint(0, 100) == 1: print('epoch {}, loss {}'.format(epoch,loss.item()))\n",
    "\n",
    "        print(loss.data)\n",
    "        \n",
    "        test = Variable(torch.Tensor(X_test), requires_grad=False)\n",
    "        predicted_proba = model.forward(test)\n",
    "        predicted_proba = predicted_proba.exp().detach().data.numpy()\n",
    "\n",
    "        label_list = model.forward(Variable(torch.Tensor(X_train), requires_grad=False)).exp().detach().data.numpy()\n",
    "        regressor.fit(label_list, r_target_np[train_index])\n",
    "\n",
    "        c_predictions = model.forward(Variable(torch.Tensor(X_test), requires_grad=False)).exp().detach().data.numpy()\n",
    "        r_predictions = regressor.predict(c_predictions)\n",
    "        mae = get_regressor_mae(r_predictions, r_target_np[test_index])\n",
    "        mae_list.append(mae)\n",
    "        '''\n",
    "        predicted = []\n",
    "        for probas in predicted_proba:\n",
    "            point = np.argmax(probas)\n",
    "            predicted.append(point)\n",
    "        KNeighborsRegressor()\n",
    "\n",
    "        mae = get_classifier_mae(predictions=predicted, real=y_test.ravel())\n",
    "        print(mae)\n",
    "        \n",
    "        train = Variable(torch.Tensor(X_train), requires_grad=False)\n",
    "        predicted_proba_train = model.forward(train)\n",
    "        predicted_proba_train = predicted_proba_train.exp().detach().data.numpy()\n",
    "        \n",
    "        predicted_train = []\n",
    "        for probas in predicted_proba_train:\n",
    "            point = np.argmax(probas)\n",
    "            predicted_train.append(point)\n",
    "\n",
    "        mae_train = get_classifier_mae(predictions=predicted_train, real=y_train.ravel())\n",
    "        \n",
    "        mae_list.append(mae)\n",
    "        mae_list_train.append(mae_train)\n",
    "        '''\n",
    "        k_index += 1\n",
    "\n",
    "    return mae_list, mae_list_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classi_nn_params():\n",
    "    model = LinearClassificationModel(4, 543)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.NLLLoss()\n",
    "    l_rate = 0.01\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr = l_rate) #Stochastic Gradient Descent\n",
    "    #optimizer = torch.optim.Adam(model.parameters(),lr=l_rate,weight_decay=1e-4)\n",
    "    epochs = 1000\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'l_rate': l_rate,\n",
    "        'optimiser': optimiser,\n",
    "        'criterion': criterion,\n",
    "        'epochs': epochs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_cross_validation_classi(build_classi_nn_params, classi_data_np, classi_target_np, regre_target_np, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
