{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.sql.SparkSession.Builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levanto los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from IPython.display import display, HTML\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils.emissions_normalizer import EmissionsNormalizer\n",
    "from utils.neural_network_predictor import NeuralNetworkPredictor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_recep = sc.read.json('datos/train-test-by-emission.jsonlines/').rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p1, p2):\n",
    "    return math.sqrt((p1[0]-p2[0])**2+(p1[1]-p2[1])**2)\n",
    "\n",
    "def get_regressor_mae(predictions, real):\n",
    "    mae = 0\n",
    "    for i in range(len(predictions)):\n",
    "        mae += distance(predictions[i], real[i])\n",
    "    mae = mae/len(predictions)\n",
    "    return mae\n",
    "\n",
    "\n",
    "global dict_coordenadas\n",
    "dict_coordenadas = points_recep.map(lambda x: (x['Punto'],(x['x'], x['y']))).collectAsMap()\n",
    "\n",
    "def get_classifier_mae(predictions, real):\n",
    "    sum_error = 0\n",
    "    count = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] not in dict_coordenadas:\n",
    "            print('predicted point dont exist {}'.format(predictions[i]))\n",
    "            continue\n",
    "        pred_position = dict_coordenadas[predictions[i]]\n",
    "        real_position = dict_coordenadas[real[i]]\n",
    "        sum_error += distance(pred_position, real_position)\n",
    "        count += 1\n",
    "    return sum_error/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = EmissionsNormalizer()\n",
    "data = normalizer.normalize(points_recep)\n",
    "regre_data, regre_target = normalizer.get_regression_dataframes(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales para regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'datos/tmp/nn-model-2h'\n",
    "def k_cross_validation(predictor, data, target, k=5, init_from_file = False, save_in_file = False):   \n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    mae_list = []\n",
    "    mae_list_train = []\n",
    "    k_index = 0\n",
    "        \n",
    "    for train_index, test_index in kf.split(data):\n",
    "        predictor.clear()\n",
    "        if init_from_file:\n",
    "            predictor.open_model(MODEL_PATH)\n",
    "\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "        \n",
    "        predictor.fit(X_train, X_test)\n",
    "\n",
    "        \n",
    "        predicted = predictor.predict(X_test)\n",
    "        mae = get_regressor_mae(predictions=predicted, real=y_test)\n",
    "        print(mae)\n",
    "             \n",
    "        predicted_train = predictor.predict(X_train)\n",
    "        mae_train = get_regressor_mae(predictions=predicted_train, real=y_train)\n",
    "        print(mae_train)\n",
    "        \n",
    "        mae_list.append(mae)\n",
    "        mae_list_train.append(mae_train)\n",
    "        if save_in_file: predictor.save_model(MODEL_PATH)\n",
    "\n",
    "    return mae_list, mae_list_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([   0,    1,    3,    4,    5,    6,    8,    9,   10,   11,\\n            ...\\n            5432, 5433, 5435, 5437, 5438, 5439, 5440, 5441, 5442, 5444],\\n           dtype='int64', length=4356)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1e33f2df7ecd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNeuralNetworkPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregre_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregre_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_from_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_in_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-f15b98bce4fb>\u001b[0m in \u001b[0;36mk_cross_validation\u001b[0;34m(predictor, data, target, k, init_from_file, save_in_file)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([   0,    1,    3,    4,    5,    6,    8,    9,   10,   11,\\n            ...\\n            5432, 5433, 5435, 5437, 5438, 5439, 5440, 5441, 5442, 5444],\\n           dtype='int64', length=4356)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "k_cross_validation(NeuralNetworkPredictor(), regre_data, regre_target, k=5, init_from_file=False, save_in_file=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales para clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_target = pd.DataFrame(all_emissions.map(lambda x: x['point']).collect())\n",
    "classi_data = pd.DataFrame(all_emissions.map(lambda x: x['data']).collect())\n",
    "\n",
    "\n",
    "classi_data_np = np.array(classi_data.astype(float))\n",
    "classi_target_np = np.array(classi_target.astype(int))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(classi_data_np, classi_target_np, test_size=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "output_dim = 543\n",
    "\n",
    "model = LinearClassificationModel(input_dim,output_dim)\n",
    "\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "l_rate = 0.01\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr = l_rate) #Stochastic Gradient Descent\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr=l_rate,weight_decay=1e-4)\n",
    "\n",
    "\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch +=1\n",
    "    #increase the number of epochs by 1 every time\n",
    "    \n",
    "    inputs = Variable(torch.Tensor(X_train), requires_grad=False)\n",
    "    labels = Variable(torch.Tensor(y_train).long(), requires_grad=False)\n",
    "    \n",
    "\n",
    "    #clear grads as discussed in prev post\n",
    "    optimiser.zero_grad()\n",
    "    #forward to get predicted values\n",
    "    outputs = model.forward(inputs)\n",
    "    loss = criterion(outputs, labels.view(-1))\n",
    "    loss.backward()# back props\n",
    "    optimiser.step()# update the parameters\n",
    "    print('epoch {}, loss {}'.format(epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Variable(torch.Tensor(X_test), requires_grad=False)\n",
    "predicted_proba = model.forward(test)\n",
    "predicted_proba = predicted_proba.exp().detach().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for probas in predicted_proba:\n",
    "    point = np.argmax(probas)\n",
    "    predicted.append(point)\n",
    "\n",
    "get_classifier_mae(predictions=predicted, real=y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classi_target_np[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.unique(classi_target_np, axis=0) == classi_target_np[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_cross_validation_classi(model_builder, data, target, r_target_np, k=5):   \n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    mae_list = []\n",
    "    mae_list_train = []\n",
    "    k_index = 0\n",
    "    \n",
    "    unique_target = np.unique(target, axis=0)\n",
    "    groups = []\n",
    "    for h in range(len(target)):\n",
    "        i,j = np.where(unique_target == target[h])\n",
    "        groups.append(i[0])\n",
    "    \n",
    "    for train_index, test_index in kf.split(data, groups=groups):\n",
    "        regressor = KNeighborsRegressor()\n",
    "        params = model_builder()\n",
    "        model = params['model']\n",
    "        l_rate = params['l_rate']\n",
    "        optimiser = params['optimiser']\n",
    "        criterion = params['criterion']\n",
    "        epochs = params['epochs']\n",
    "\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            epoch +=1\n",
    "            #increase the number of epochs by 1 every time\n",
    "\n",
    "            inputs = Variable(torch.Tensor(X_train), requires_grad=False)\n",
    "            labels = Variable(torch.Tensor(y_train).long(), requires_grad=False)\n",
    "\n",
    "\n",
    "            #clear grads as discussed in prev post\n",
    "            optimiser.zero_grad()\n",
    "            #forward to get predicted values\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels.view(-1))\n",
    "            loss.backward()# back props\n",
    "            optimiser.step()# update the parameters\n",
    "            if k_index == 0 and epoch < 1000: print('epoch {}, loss {}'.format(epoch,loss.item()))\n",
    "            if k_index == 0 and epoch > 1000 and random.randint(0, 100) == 1: print('epoch {}, loss {}'.format(epoch,loss.item()))\n",
    "\n",
    "        print(loss.data)\n",
    "        \n",
    "        test = Variable(torch.Tensor(X_test), requires_grad=False)\n",
    "        predicted_proba = model.forward(test)\n",
    "        predicted_proba = predicted_proba.exp().detach().data.numpy()\n",
    "\n",
    "        label_list = model.forward(Variable(torch.Tensor(X_train), requires_grad=False)).exp().detach().data.numpy()\n",
    "        regressor.fit(label_list, r_target_np[train_index])\n",
    "\n",
    "        c_predictions = model.forward(Variable(torch.Tensor(X_test), requires_grad=False)).exp().detach().data.numpy()\n",
    "        r_predictions = regressor.predict(c_predictions)\n",
    "        mae = get_regressor_mae(r_predictions, r_target_np[test_index])\n",
    "        mae_list.append(mae)\n",
    "        '''\n",
    "        predicted = []\n",
    "        for probas in predicted_proba:\n",
    "            point = np.argmax(probas)\n",
    "            predicted.append(point)\n",
    "        KNeighborsRegressor()\n",
    "\n",
    "        mae = get_classifier_mae(predictions=predicted, real=y_test.ravel())\n",
    "        print(mae)\n",
    "        \n",
    "        train = Variable(torch.Tensor(X_train), requires_grad=False)\n",
    "        predicted_proba_train = model.forward(train)\n",
    "        predicted_proba_train = predicted_proba_train.exp().detach().data.numpy()\n",
    "        \n",
    "        predicted_train = []\n",
    "        for probas in predicted_proba_train:\n",
    "            point = np.argmax(probas)\n",
    "            predicted_train.append(point)\n",
    "\n",
    "        mae_train = get_classifier_mae(predictions=predicted_train, real=y_train.ravel())\n",
    "        \n",
    "        mae_list.append(mae)\n",
    "        mae_list_train.append(mae_train)\n",
    "        '''\n",
    "        k_index += 1\n",
    "\n",
    "    return mae_list, mae_list_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classi_nn_params():\n",
    "    model = LinearClassificationModel(4, 543)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.NLLLoss()\n",
    "    l_rate = 0.01\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr = l_rate) #Stochastic Gradient Descent\n",
    "    #optimizer = torch.optim.Adam(model.parameters(),lr=l_rate,weight_decay=1e-4)\n",
    "    epochs = 1000\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'l_rate': l_rate,\n",
    "        'optimiser': optimiser,\n",
    "        'criterion': criterion,\n",
    "        'epochs': epochs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_cross_validation_classi(build_classi_nn_params, classi_data_np, classi_target_np, regre_target_np, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
